#   "bark_thick_ln" = c(-2.564, 0, -0.659, -0.611, -1.917, -0.495, -2.504, -0.945, 0.318, -0.293, -1.231, -0.647, -0.789, 1.5, 1.133),
#   "intercept" = c(0.599, 0, 0.425, 0.413, 0.503, 0.316, 0.703, 0.396, 0.295, 0.385, 0.526, 0.423, 0.341, 0.053, -0.057))
#
# bark$bark_thick <- ifelse(bark$bark_thick_ln != 0, exp(bark$bark_thick_ln), bark$bark_thick_ln)
#
# bark$bark_thick <- exp(bark$bark_thick_ln)
#
# dbh$bark_thick <- bark$bark_thick[match(dbh$sp, bark$sp)]
# dbh$intercept <- bark$intercept[match(dbh$sp, bark$sp)]
#
# #the main equation is based on ring widths. We have determined the equation to be
# # rw(pointer_year) <- 0.5*dbh2013 - bark_thick*(dbh2013^intercept) - sum(rw(pointer_year):rw(end)). The first part of the equation is here. Summing the pointer years happens with the "q" df below in the loop.
# dbh$rw_prelim <- (0.5*dbh$dbh2013) - (dbh$bark_thick*(dbh$dbh2013^dbh$intercept))
#
#
# dbh$dbh_old.mm <- "0" #in prep for below
# dbh$dbh_old.mm <- as.numeric(dbh$dbh_old.mm)
#
# for (i in seq(along=widths)){
#   df <- widths[[i]] #the list "widths" comes from #4a-4b
#   colnames(df) <- gsub("A", "", colnames(df)) #remove "A"
#   colnames(df) <- gsub("^0", "", colnames(df)) #remove leading 0
#
#   cols <- colnames(df) #define cols for below
#   colnames(df) <- gsub("^", "x", colnames(df)) #add "x" to make calling colnames below feasible
#
#   for (j in seq(along=cols)){
#     for (k in seq(along=colnames(df))){
#       ring_ind <- cols[[j]]
#       ring_col <- colnames(df)[[k]]
#
#       if(j==k){
#         #the output of this loop is 3 separate columns for each year's old dbh, hence why it is set to q as a dataframe before being combined below. Pointer_years_simple comes from #4d.
#         q <- data.frame(sapply(pointer_years_simple, function(x){
#           rw <- df[rownames(df)>=x, ]
#           ifelse(dbh$year == x & dbh$tree == ring_ind,
#                  dbh$rw_prelim - sum(rw[, ring_col], na.rm=TRUE), 0)
#         }))
#
#         q$dbh_old.mm <- q[,1] +q[,2] + q[,3] #add columns together
#         # q$dbh_old.mm <- q[,1] +q[,2] + q[,3] + q[,4]
#         dbh$dbh_old.mm <- dbh$dbh_old.mm + q$dbh_old.mm #combine with dbh
#       }
#     }
#   }
# }
#
# # check <- dbh[dbh$dbh_old.mm == 0, ] #check if any tree was missed
#
# trees_all$dbh_old.mm <- dbh$dbh_old.mm
# trees_all$dbh_old.mm <- ifelse(trees_all$dbh_old.mm < 0, 0, trees_all$dbh_old.mm)
# trees_all$dbh_old.mm <- ifelse(trees_all$dbh_old.mm > 0, trees_all$dbh_old.mm/10, trees_all$dbh_old.mm)
# trees_all$dbh_ln <- ifelse(trees_all$dbh_old.mm == 0, NA, log(trees_all$dbh_old.mm))
###new method ####
bark <- read.csv("data/traits/SCBI_bark_depth.csv")
bark <- bark[bark$species %in% sp_can | bark$species %in% sp_subcan, ]
#1. Calculate diameter_nobark for 2008 = DBH.mm.2008-2*bark.depth.mm
bark$diam_nobark_2008.mm <- bark$DBH.mm.2008 - 2*bark$bark.depth.mm
#2. log-transform both diam_nobark_2008 (x) and bark.depth.mm (y)
#3. Fit a linear model, and use model to predict log(bark.depth.mm)
source_gist("524eade46135f6348140")
ggplot(data = bark, aes(x = log(diam_nobark_2008.mm), y = log(bark.depth.mm))) +
stat_smooth_func(geom="text",method="lm",hjust=0.16, vjust=-1,parse=TRUE) +
geom_smooth(method="lm", se=FALSE, color="black") +
geom_point(color = "#0c4c8a") +
theme_minimal() +
facet_wrap(vars(species))
#this full equation is used further down
ggplot(data = bark, aes(x = log(diam_nobark_2008.mm), y = log(bark.depth.mm))) +
stat_smooth_func(geom="text",method="lm",hjust=0.16, vjust=-1,parse=TRUE) +
geom_smooth(method="lm", se=FALSE, color="black") +
geom_point(color = "#0c4c8a") +
theme_minimal()
#no total regression equation at bottom because all species are accounted for in dataset.
bark$predict_barkthick.ln.mm <- NA
bark$predict_barkthick.ln.mm <-
ifelse(bark$species == "caco", -1.56+0.416*log(bark$diam_nobark_2008.mm),
ifelse(bark$species == "cagl", -0.393+0.268*log(bark$diam_nobark_2008.mm),
ifelse(bark$species == "caovl", -2.18+0.651*log(bark$diam_nobark_2008.mm),
ifelse(bark$species == "cato", -0.477+0.301*log(bark$diam_nobark_2008.mm),
ifelse(bark$species == "fram", 0.418+0.268*log(bark$diam_nobark_2008.mm),
ifelse(bark$species == "juni", 0.346+0.279*log(bark$diam_nobark_2008.mm),
ifelse(bark$species == "litu", -1.14+0.463*log(bark$diam_nobark_2008.mm),
ifelse(bark$species == "qual", -2.09+0.637*log(bark$diam_nobark_2008.mm),
ifelse(bark$species == "qupr", -1.31+0.528*log(bark$diam_nobark_2008.mm),
ifelse(bark$species == "quru", -0.593+0.292*log(bark$diam_nobark_2008.mm),
ifelse(bark$species == "quve", 0.245+0.219*log(bark$diam_nobark_2008.mm),
bark$predict_barkthick.ln)))))))))))
#4. Take exponent of bark.depth.mm and make sure predicted values look good.
bark$predict_barkthick.mm <- exp(bark$predict_barkthick.ln.mm)
range(bark$predict_barkthick.mm - bark$bark.depth.mm)
#5. Get mean bark thickness per species in 2008.
## The equation for calculating old dbh, using 1999 as an example, is
## dbh1999 = dbh2008 - 2(ring.width2013 - ring.width1999) - 2(bark.depth2008) + 2(bark.depth1999)
## using the dataset from calculating the regression equations, we can get mean bark thickness per species in 2008.
##set up dbh dataframe
dbh <- trees_all[, c(1:4)]
scbi.stem1 <- read.csv(text=getURL("https://raw.githubusercontent.com/SCBI-ForestGEO/SCBI-ForestGEO-Data/master/tree_main_census/data/census-csv-files/scbi.stem1.csv"))
dbh$dbh2008.mm <- scbi.stem1$dbh[match(dbh$tree, scbi.stem1$tag)]
mean_bark <- aggregate(bark$bark.depth.mm, by=list(bark$species), mean) #mm
colnames(mean_bark) <- c("sp", "mean_bark_2008.mm")
dbh$mean_bark_2008.mm <- ifelse(dbh$sp %in% mean_bark$sp, mean_bark$mean_bark_2008.mm[match(dbh$sp, mean_bark$sp)], mean(bark$bark.depth.mm))
dbh$mean_bark_2008.mm <- round(dbh$mean_bark_2008.mm, 2)
#6.Thus, the only value we're missing is bark depth in 1999.
## This is ok, because we can calculate from the regression equation per each species (all we need is diam_nobark_1999).Calculate diam_nobark_1999 using
## diam_nobark_1999 = dbh2008 - 2*(bark.depth2008) - 2*(sum(ring.width1999:ring.width2008))
##define this column before loop
dbh$diam_nobark_old.mm <- 0
for (i in seq(along=widths)){
df <- widths[[i]] #the list "widths" comes from #4a-4b
colnames(df) <- gsub("A", "", colnames(df)) #remove "A"
colnames(df) <- gsub("^0", "", colnames(df)) #remove leading 0
cols <- colnames(df) #define cols for below
colnames(df) <- gsub("^", "x", colnames(df)) #add "x" to make calling colnames below feasible
for (j in seq(along=cols)){
for (k in seq(along=colnames(df))){
ring_ind <- cols[[j]]
ring_col <- colnames(df)[[k]]
if(j==k){
#the output of this loop is 3 separate columns for each year's old dbh, hence why it is set to q as a dataframe before being combined below. Pointer_years_simple comes from #4d.
q <- data.frame(sapply(pointer_years_simple, function(x){
rw <- df[rownames(df)>=x, ]
ifelse(dbh$year == x & dbh$tree == ring_ind,
dbh$dbh2008.mm - 2*(dbh$mean_bark_2008.mm) - sum(rw[rownames(rw) %in% c(x:2008), ring_col], na.rm=TRUE), 0)
}))
q$diam_nobark_old.mm <- q[,1] +q[,2] + q[,3] #add columns together
# q$dbh_old.mm <- q[,1] +q[,2] + q[,3] + q[,4]
dbh$diam_nobark_old.mm <- dbh$diam_nobark_old.mm + q$diam_nobark_old.mm #combine with dbh (it's the same order of rows) #mm
}
}
}
}
#7. Calculate bark thickness using regression equation per appropriate sp
## log(bark.depth.1999) = intercept + log(diam_nobark)*constant
## bark.depth.1999 = exp(log(bark.depth.1999))
#the full equation at the bottom is the regression equation for all these species put together. "fagr" is given a bark thickness of 0 because it is negligble
#these equations are the same as above in #3 of this code section
dbh$bark_thick_old.ln.mm <- NA
dbh$bark_thick_old.ln.mm <- ifelse(dbh$sp == "caco", -1.56+0.416*log(dbh$diam_nobark_old.mm),
ifelse(dbh$sp == "cagl", -0.393+0.268*log(dbh$diam_nobark_old.mm),
ifelse(dbh$sp == "caovl", -2.18+0.651*log(dbh$diam_nobark_old.mm),
ifelse(dbh$sp == "cato", -0.477+0.301*log(dbh$diam_nobark_old.mm),
ifelse(dbh$sp == "fram", 0.418+0.268*log(dbh$diam_nobark_old.mm),
ifelse(dbh$sp == "juni", 0.346+0.279*log(dbh$diam_nobark_old.mm),
ifelse(dbh$sp == "litu", -1.14+0.463*log(dbh$diam_nobark_old.mm),
ifelse(dbh$sp == "qual", -2.09+0.637*log(dbh$diam_nobark_old.mm),
ifelse(dbh$sp == "qupr", -1.31+0.528*log(dbh$diam_nobark_old.mm),
ifelse(dbh$sp == "quru", -0.593+0.292*log(dbh$diam_nobark_old.mm),
ifelse(dbh$sp == "quve", 0.245+0.219*log(dbh$diam_nobark_old.mm),
ifelse(dbh$sp == "fagr", 0,
-1.01+0.213*log(dbh$diam_nobark_old.mm)))))))))))))
dbh$bark_thick_old.mm <- ifelse(dbh$sp == "fagr", 0, exp(dbh$bark_thick_old.ln.mm))
#8. Add to solution from #6 to get full dbh1999
## dbh1999 = diam_nobark_1999 + 2*bark.depth.1999
dbh$dbh_old.mm <- dbh$diam_nobark_old.mm + 2*dbh$bark_thick_old.mm
##NOTE
##The first time I ran this code I was getting NaNs for one tree (140939), because the dbh in 2008 was listed as 16.9. I double-checked this, and that was the second stem, which we obviously didn't core at a size of 1.69 cm (or 2.2 cm in 2013). The dbh is meant to be the first stem. However, there was confusion with the dbh in the field.
trees_all$dbh_old.mm <- dbh$dbh_old.mm[match(trees_all$tree, dbh$tree) & match(trees_all$year, dbh$year)] #mm
trees_all$dbh_old.cm <- trees_all$dbh_old.mm/10
trees_all$dbh.ln.cm <- log(trees_all$dbh_old.cm)
##5f. add in ratio of sapwood area to total wood ####
### It has been determined that since sapwood ratio is so tied to DBH (in other words, testing it in a model is akin to testing DBH again), that we are going to leave it out of the full models. However, I'm leaving the code here in case we want anything with it later.
#calculate sapwood area
##sapwood area[iii] = tree area (minus bark)[i] - heartwood area[ii]
sap <- read.csv("data/traits/SCBI_Sapwood_Data.csv", stringsAsFactors = FALSE)
sap <- sap[,c(1:5,8:10,24)]
sap$sp <- paste0(gsub("^(..).*", "\\1", sap$Latin),
gsub("^.* (..).*", "\\1", sap$Latin))
sap$sp <- tolower(sap$sp)
##subtract bark thickness from dbh
##NOTE bark thickness is from 2008, even tho sap data collected 2010
##[[i]]
sap$dbh_nobark.mm <- 0
for (i in seq(along=mean_bark$mean_bark_2008)){
sub <- mean_bark[mean_bark$mean_bark_2008[[i]] == mean_bark$mean_bark_2008, ]
sap$dbh_nobark.mm <- ifelse(sap$sp == sub$sp, sap$DBH-sub$mean_bark_2008, sap$dbh_nobark)
}
#[ii]
#heartwood radius = 0.5*dbh – sapwood depth (mm)
sap$hw_rad.mm <- 0.5*sap$dbh_nobark.mm - sap$sapwood.depth..mm.
#Heartwood area = pi*(heartwood radius)^2 (mm^2)
sap$hw_area.mm2 <- pi*(sap$hw_rad.mm)^2
#[iii]
#Sapwood area = pi*((0.5*dbh)^2) – heartwood area (cm^2 with the /100)
sap$sap_area.cm2 <- (pi*(0.5*sap$dbh_nobark.mm)^2 - sap$hw_area.mm2)/100
sap <- sap[sap$sp %in% sp_can | sap$sp %in% sp_subcan, ]
#ratio = sapwood area:total wood area (without bark)
##calculate ratio to find the regression equations
sap$dbh_nobark.cm <- sap$dbh_nobark.mm/10
sap$total_wood_area.cm2 <- pi*(sap$dbh_nobark.cm/2)^2
sap$sap_ratio <- sap$sap_area.cm2/sap$total_wood_area.cm2
#these equations are for getting the sapwood_area for the dbh df, which are then converted to sap_ratio for that df.
source_gist("524eade46135f6348140")
#DBH = mm, sap_area = cm^2
ggplot(data = sap, aes(x = log(DBH), y = log(sap_area.cm2))) +
stat_smooth_func(geom="text",method="lm",hjust=0.16, vjust=0.8,parse=TRUE) +
geom_smooth(method="lm", se=FALSE, color="black") +
geom_point(color = "#0c4c8a") +
theme_minimal() +
facet_wrap(vars(sp))
ggplot(data = sap, aes(x = log(DBH), y = log(sap_area.cm2))) +
stat_smooth_func(geom="text",method="lm",hjust=0.16, vjust=-1,parse=TRUE) +
geom_smooth(method="lm", se=FALSE, color="black") +
geom_point(color = "#0c4c8a") +
theme_minimal()
#original equations. I'm not sure what happened, but I think after changing some units, I forgot to update these. Nevertheless, I'm leaving them as is in case I messed up.
# #the bottom equation is the total regression equation
# dbh$sapwood_area.ln <- NA
# dbh$sapwood_area.ln <- ifelse(dbh$sp == "caco", 6.17-0.419*log(dbh$dbh_old.mm),
#                        ifelse(dbh$sp == "cagl", 5.32-0.26*log(dbh$dbh_old.mm),
#                        ifelse(dbh$sp == "cato", 6.51-0.444*log(dbh$dbh_old.mm),
#                        ifelse(dbh$sp == "fram", 2.19+0.326*log(dbh$dbh_old.mm),
#                        ifelse(dbh$sp == "juni", 5.53-0.404*log(dbh$dbh_old.mm),
#                        ifelse(dbh$sp == "litu", 4.31-0.0718*log(dbh$dbh_old.mm),
#                        ifelse(dbh$sp == "qual", 7.09-0.692*log(dbh$dbh_old.mm),
#                        ifelse(dbh$sp == "qupr", 4.99-0.305*log(dbh$dbh_old.mm),
#                        ifelse(dbh$sp == "quru", 4.27-0.282*log(dbh$dbh_old.mm),
#                        ifelse(dbh$sp == "quve", 5.1-0.402*log(dbh$dbh_old.mm),
#                                    6.6-0.543*log(dbh$dbh_old.mm)))))))))))
#the bottom equation is the total regression equation
dbh$sapwood_area.ln <- NA
dbh$sapwood_area.ln <- ifelse(dbh$sp == "caco", -3.41+1.6*log(dbh$dbh_old.mm),
ifelse(dbh$sp == "cagl", -4.34+1.77*log(dbh$dbh_old.mm),
ifelse(dbh$sp == "cato", -3.14+1.59*log(dbh$dbh_old.mm),
ifelse(dbh$sp == "fram", -7.75+2.4*log(dbh$dbh_old.mm),
ifelse(dbh$sp == "juni", -4.23+1.64*log(dbh$dbh_old.mm),
ifelse(dbh$sp == "litu", -5.5+1.98*log(dbh$dbh_old.mm),
ifelse(dbh$sp == "qual", -2.66+1.35*log(dbh$dbh_old.mm),
ifelse(dbh$sp == "qupr", -4.89+1.76*log(dbh$dbh_old.mm),
ifelse(dbh$sp == "quru", -5.35+1.74*log(dbh$dbh_old.mm),
ifelse(dbh$sp == "quve", -4.57+1.63*log(dbh$dbh_old.mm),
-3.13+1.5*log(dbh$dbh_old.mm)))))))))))
dbh$sapwood_area.cm2 <- exp(dbh$sapwood_area.ln)
#calculate ratio for each tree using regression equations
##prepare: get radius.w/o.bark (/2) and convert to cm (/10).
dbh$radius_nobark.cm <- (dbh$diam_nobark_old/2)/10
##total wood area = pi*(radius.nobark)^2 (cm^2)
dbh$total_wood_area.cm2 <- pi*(dbh$radius_nobark.cm)^2
dbh$sap_ratio <- dbh$sapwood_area.cm2/dbh$total_wood_area.cm2
trees_all$sap_ratio <- dbh$sap_ratio[match(trees_all$tree, dbh$tree) & match(trees_all$year, dbh$year)]
##5g. add in tree heights ####
## taken from the canopy_heights script
#the full equation is using all points for which we have data to create the equation, despite that for several species we don't have enough data to get a sp-specific equation
trees_all$height.ln.m <- ifelse(trees_all$sp == "caco", (0.348+0.808*trees_all$dbh.ln.cm),
ifelse(trees_all$sp == "cagl", (0.681+0.704*trees_all$dbh.ln.cm),
ifelse(trees_all$sp == "caovl", (0.621+0.722*trees_all$dbh.ln.cm),
ifelse(trees_all$sp == "cato", (0.776+0.701*trees_all$dbh.ln.cm),
ifelse(trees_all$sp == "fagr", (0.708+0.662*trees_all$dbh.ln.cm),
ifelse(trees_all$sp == "fram", (0.715+0.619*trees_all$dbh.ln.cm),
ifelse(trees_all$sp == "juni", (1.22+0.49*trees_all$dbh.ln.cm),
ifelse(trees_all$sp == "litu", (1.32+0.524*trees_all$dbh.ln.cm),
ifelse(trees_all$sp == "qual", (1.14+0.548*trees_all$dbh.ln.cm),
ifelse(trees_all$sp == "qupr", (0.44+0.751*trees_all$dbh.ln.cm),
ifelse(trees_all$sp == "quru", (1.17+0.533*trees_all$dbh.ln.cm),
ifelse(trees_all$sp == "quve", (0.864+0.585*trees_all$dbh.ln.cm),
(0.791+0.645*trees_all$dbh.ln.cm)))))))))))))
#0.849+0.659*trees_all$dbh_ln.cm -> original equation that was yielding predicted heights for some trees of about 54m.
trees_all$height.m <- exp(trees_all$height.ln.m) #m, because these equations come from a plotting of log(DBH in cm) against log(height in m).
#cap values at max for different species.
# heights_full <- read.csv(text=getURL("https://raw.githubusercontent.com/SCBI-ForestGEO/SCBI-ForestGEO-Data/master/tree_dimensions/tree_heights/SCBI_tree_heights.csv"), stringsAsFactors = FALSE)
#
# max_ht <- aggregate(height.m ~ sp, data=heights_full, FUN=max)
##5h. add in all crown positions ####
positions <- read.csv(text=getURL("https://raw.githubusercontent.com/SCBI-ForestGEO/SCBI-ForestGEO-Data/master/tree_dimensions/tree_crowns/cored_dendroband_crown_position_data/dendro_cored_full.csv"))
trees_all$position_all <- positions$crown.position[match(trees_all$tree, positions$tag)]
trees_all$position_all <- gsub("D", "dominant", trees_all$position_all)
trees_all$position_all <- gsub("C", "co-dominant", trees_all$position_all)
trees_all$position_all <- gsub("I", "intermediate", trees_all$position_all)
trees_all$position_all <- gsub("S", "suppressed", trees_all$position_all)
#this has been proven to be roughly equivalent to position_all, so we're sticking with position_all
# trees_all$illum <- positions$illum[match(trees_all$tree, positions$tag)]
# trees_all$illum <- as.character(trees_all$illum)
#this csv has avg/min/max dbh for each canopy position by sp
# positionsp <- read.csv("data/core_chronologies_by_crownposition.csv")
##5i. remove all NAs and one bad tree ####
##fram 140939 has been mislabeled. It is recorded as having a small dbh when that is the second stem. In terms of canopy position, though, it fell between time of coring and when positions were recorded, thus we do not know its position.
trees_all <- trees_all[!trees_all$tree == 140939, ]
trees_all <- trees_all[trees_all$resist.value <=2,]
##5j. subset for either leaf hydraulic traits or biophysical ####
trees_all_traits <- trees_all[complete.cases(trees_all), ]
trees_all_bio <- trees_all[c("year", "sp", "tree", "resist.value", "elev.m", "distance.ln.m", "height.ln.m", "position_all"), ]
View(trees_all_bio)
trees_all_bio <- trees_all[c("year", "sp", "tree", "resist.value", "elev.m", "distance.ln.m", "height.ln.m", "position_all")]
trees_all_bio <- trees_all_bio[complete.cases(trees_all_bio), ]
library(lme4)
library(AICcmodavg) #aictab function
library(car)
library(piecewiseSEM) #for R^2 values for all model outputs in a list
library(MuMIn) #for R^2 values of one model output
library(stringr)
library(dplyr)
##6aiii. base code for running multiple models through AICc eval ####
#define response and effects
response <- "resist.value"
# effects <- c("position_all", "elev.m", "distance.ln.m", "height.ln.m", "year", "(1|sp/tree)")
effects <- c("tlp", "rp", "PLA_dry_percent", "LMA_g_per_m2", "Chl_m2_per_g", "WD_g_per_cm3", "p50.MPa", "p80.Mpa", "year", "(1|sp/tree)")
#create all combinations of random / fixed effects
effects_comb <-
unlist( sapply( seq_len(length(effects)),
function(i) {
apply( combn(effects,i), 2, function(x) paste(x, collapse = "+"))
}))
# pair response with effect and sub out combinations that don't include random effects
#in general, if two variables are >70% correlated, you can toss one of them without significantly affecting the results
var_comb <- expand.grid(response, effects_comb)
var_comb <- var_comb[grepl("1", var_comb$Var2), ] #only keep in fixed/random combos
var_comb <- var_comb[grepl("year", var_comb$Var2), ] #keep year in for drought sake
# formulas for all combinations. $Var1 is the response, and $Var2 is the effect
# for good stats, you should have no more total parameters than 1/10th the number of observations in your dataset
formula_vec <- sprintf("%s ~ %s", var_comb$Var1, var_comb$Var2)
# create list of model outputs
lmm_all <- lapply(formula_vec, function(x){
fit1 <- lmer(x, data = trees_all_traits, REML=FALSE,
control = lmerControl(optimizer ="Nelder_Mead"))
return(fit1)
})
View(trees_all_traits)
# effects <- c("position_all", "elev.m", "distance.ln.m", "height.ln.m", "year", "(1|sp/tree)")
effects <- c("rp", "PLA_dry_percent", "LMA_g_per_m2", "Chl_m2_per_g", "WD_g_per_cm3", "mean_TLP_Mpa", "p50.MPa", "p80.MPa", "year", "(1|sp/tree)")
#create all combinations of random / fixed effects
effects_comb <-
unlist( sapply( seq_len(length(effects)),
function(i) {
apply( combn(effects,i), 2, function(x) paste(x, collapse = "+"))
}))
# pair response with effect and sub out combinations that don't include random effects
#in general, if two variables are >70% correlated, you can toss one of them without significantly affecting the results
var_comb <- expand.grid(response, effects_comb)
var_comb <- var_comb[grepl("1", var_comb$Var2), ] #only keep in fixed/random combos
var_comb <- var_comb[grepl("year", var_comb$Var2), ] #keep year in for drought sake
# formulas for all combinations. $Var1 is the response, and $Var2 is the effect
# for good stats, you should have no more total parameters than 1/10th the number of observations in your dataset
formula_vec <- sprintf("%s ~ %s", var_comb$Var1, var_comb$Var2)
# create list of model outputs
lmm_all <- lapply(formula_vec, function(x){
fit1 <- lmer(x, data = trees_all_traits, REML=FALSE,
control = lmerControl(optimizer ="Nelder_Mead"))
return(fit1)
})
names(lmm_all) <- formula_vec
var_aic <- aictab(lmm_all, second.ord=TRUE, sort=TRUE) #rank based on AICc
View(var_aic)
##6aii. coefficients ####
best <- lmm_all[[123]]
coef(summary(best))[ , "Estimate"]
##6aii. coefficients ####
best <- lmm_all[[129]]
coef(summary(best))[ , "Estimate"]
var_comb
View(var_comb)
r <- rsquared(lmm_all) #gives R^2 values for models. "Marginal" is the R^2 for just the fixed effects, "Conditional" is the R^2 for everything.
View(r)
census3 <- read,csv(text=getURL("https://raw.githubusercontent.com/SCBI-ForestGEO/SCBI-ForestGEO-Data_private/master/census%20data/ViewFullTable_crc_master.csv?token=AJNRBEOJRZXIQMJYEZTEVT25BEYQS"))
census3 <- read.csv(text=getURL("https://raw.githubusercontent.com/SCBI-ForestGEO/SCBI-ForestGEO-Data_private/master/census%20data/ViewFullTable_crc_master.csv?token=AJNRBEOJRZXIQMJYEZTEVT25BEYQS"))
census3$position.crown <- positions$crown.position[match(census3$StemID, positions$stemID)]
census3_sub <- census3[!is.na(census3$position.crown), ]
ggplot(census3) +
geom_bar(aes(weight=crown.position))+
theme_minimal()
ggplot(census3) +
geom_bar(weight=crown.position)+
theme_minimal()
ggplot(census3_sub) +
geom_bar(weight=position.crown)+
theme_minimal()
library(esquisse)
esquisser()
esquisser()
View(census3_sub)
census3_sub$DBH <- as.numeric(census3$DBH)
census3_sub$DBH <- as.numeric(census3_sub$DBH)
esquisser()
ggplot(data = census3_sub) +
aes(x = DBH, fill = position.crown) +
geom_histogram(bins = 61) +
theme_minimal() +
facet_wrap(vars(position.crown), ncol=1)
census3_sub <- census3[!is.na(census3$position.crown) & census3_sub$CensusID == 3 & census3_sub$DBH != "NULL", ]
census3_sub <- census3[!(is.na(census3$position.crown)) & census3_sub$CensusID == 3 & census3_sub$DBH != "NULL", ]
census3_sub <- census3[!(is.na(census3$position.crown)) & census3_sub$CensusID == 3 & !(grepl("D", census3_sub$ListOfTSM)), ]
!(is.na(census3$position.crown))
census3_sub <- census3[!(is.na(census3$position.crown)) & census3$CensusID == 3 & !(grepl("D", census3$ListOfTSM)), ]
ggplot(data = census3_sub) +
aes(x = DBH, fill = position.crown) +
geom_histogram(bins = 61) +
theme_minimal() +
facet_wrap(vars(position.crown), ncol=1)
census3_sub$DBH <- as.numeric(census3$DBH)
census3_sub$DBH <- as.numeric(census3_sub$DBH)
ggplot(data = census3_sub) +
aes(x = DBH, fill = position.crown) +
geom_histogram(bins = 61) +
theme_minimal() +
facet_wrap(vars(position.crown), ncol=1)
ggplot(data = census3_sub) +
aes(x = DBH, fill = position.crown) +
geom_histogram(bins = 61) +
theme_minimal()
ggplot(data = census3_sub) +
aes(x = DBH, fill = position.crown) +
geom_histogram(bins = 30) +
theme_minimal() +
facet_wrap(vars(position.crown), ncol=1)
ggplot(data = census3_sub) +
aes(x = DBH, fill = position.crown) +
geom_histogram(bins = 30) +
theme_minimal()
##5k. make subsets for individual years, combine all to list ####
x1966 <- trees_all_bio[trees_all_bio$year == 1966, ]
x1977 <- trees_all_bio[trees_all_bio$year == 1977, ]
x1999 <- trees_all_bio[trees_all_bio$year == 1999, ]
##6aiii. base code for running multiple models through AICc eval ####
#define response and effects
response <- "resist.value"
effects <- c("position_all", "elev.m", "distance.ln.m", "height.ln.m", "year", "(1|sp/tree)")
#create all combinations of random / fixed effects
effects_comb <-
unlist( sapply( seq_len(length(effects)),
function(i) {
apply( combn(effects,i), 2, function(x) paste(x, collapse = "+"))
}))
# pair response with effect and sub out combinations that don't include random effects
#in general, if two variables are >70% correlated, you can toss one of them without significantly affecting the results
var_comb <- expand.grid(response, effects_comb)
var_comb <- var_comb[grepl("1", var_comb$Var2), ] #only keep in fixed/random combos
var_comb <- var_comb[grepl("year", var_comb$Var2), ] #keep year in for drought sake
# formulas for all combinations. $Var1 is the response, and $Var2 is the effect
# for good stats, you should have no more total parameters than 1/10th the number of observations in your dataset
formula_vec <- sprintf("%s ~ %s", var_comb$Var1, var_comb$Var2)
# create list of model outputs
lmm_all <- lapply(formula_vec, function(x){
fit1 <- lmer(x, data = x1999, REML=FALSE,
control = lmerControl(optimizer ="Nelder_Mead"))
return(fit1)
})
effects <- c("position_all", "elev.m", "distance.ln.m", "height.ln.m", "(1|sp/tree)")
#create all combinations of random / fixed effects
effects_comb <-
unlist( sapply( seq_len(length(effects)),
function(i) {
apply( combn(effects,i), 2, function(x) paste(x, collapse = "+"))
}))
# pair response with effect and sub out combinations that don't include random effects
#in general, if two variables are >70% correlated, you can toss one of them without significantly affecting the results
var_comb <- expand.grid(response, effects_comb)
var_comb <- var_comb[grepl("1", var_comb$Var2), ] #only keep in fixed/random combos
var_comb <- var_comb[grepl("year", var_comb$Var2), ] #keep year in for drought sake
# formulas for all combinations. $Var1 is the response, and $Var2 is the effect
# for good stats, you should have no more total parameters than 1/10th the number of observations in your dataset
formula_vec <- sprintf("%s ~ %s", var_comb$Var1, var_comb$Var2)
# create list of model outputs
lmm_all <- lapply(formula_vec, function(x){
fit1 <- lmer(x, data = x1999, REML=FALSE,
control = lmerControl(optimizer ="Nelder_Mead"))
return(fit1)
})
#create all combinations of random / fixed effects
effects_comb <-
unlist( sapply( seq_len(length(effects)),
function(i) {
apply( combn(effects,i), 2, function(x) paste(x, collapse = "+"))
}))
# pair response with effect and sub out combinations that don't include random effects
#in general, if two variables are >70% correlated, you can toss one of them without significantly affecting the results
var_comb <- expand.grid(response, effects_comb)
var_comb <- var_comb[grepl("1", var_comb$Var2), ] #only keep in fixed/random combos
# formulas for all combinations. $Var1 is the response, and $Var2 is the effect
# for good stats, you should have no more total parameters than 1/10th the number of observations in your dataset
formula_vec <- sprintf("%s ~ %s", var_comb$Var1, var_comb$Var2)
# create list of model outputs
lmm_all <- lapply(formula_vec, function(x){
fit1 <- lmer(x, data = x1999, REML=FALSE,
control = lmerControl(optimizer ="Nelder_Mead"))
return(fit1)
})
View(x1999)
View(var_comb)
# create list of model outputs
lmm_all <- lapply(formula_vec, function(x){
fit1 <- lmer(x, data = x1999, REML=FALSE,
control = lmerControl(optimizer ="Nelder_Mead"))
return(fit1)
})
effects <- c("position_all", "elev.m", "distance.ln.m", "height.ln.m", "(1|sp)")
#create all combinations of random / fixed effects
effects_comb <-
unlist( sapply( seq_len(length(effects)),
function(i) {
apply( combn(effects,i), 2, function(x) paste(x, collapse = "+"))
}))
# pair response with effect and sub out combinations that don't include random effects
#in general, if two variables are >70% correlated, you can toss one of them without significantly affecting the results
var_comb <- expand.grid(response, effects_comb)
var_comb <- var_comb[grepl("1", var_comb$Var2), ] #only keep in fixed/random combos
# formulas for all combinations. $Var1 is the response, and $Var2 is the effect
# for good stats, you should have no more total parameters than 1/10th the number of observations in your dataset
formula_vec <- sprintf("%s ~ %s", var_comb$Var1, var_comb$Var2)
# create list of model outputs
lmm_all <- lapply(formula_vec, function(x){
fit1 <- lmer(x, data = x1999, REML=FALSE,
control = lmerControl(optimizer ="Nelder_Mead"))
return(fit1)
})
names(lmm_all) <- formula_vec
var_aic <- aictab(lmm_all, second.ord=TRUE, sort=TRUE) #rank based on AICc
View(var_aic)
##6aii. coefficients ####
best <- lmm_all[[4]]
coef(summary(best))[ , "Estimate"]
##6aii. coefficients ####
best <- lmm_all[[7]]
coef(summary(best))[ , "Estimate"]
